





import pandas as pd

GenderData = pd.read_csv('KGGenderDataset.csv', usecols=[0,1])
GenderData.head()


GenderData.dtypes


GenderData.isna().sum() #lihat jika ada null value


GenderData = GenderData.dropna() #drop null value


GenderData.isna().sum() #check lagi ada null value


# Function to split names
def split_name(name):

    name = name.lower()
    parts = name.split()
    
    # Initialize the result as empty strings for all parts
    first_name = second_name = third_name = fourth_name = last_name = ' '
    
    if len(parts) >= 1:
        first_name = parts[0]  # First name
    if len(parts) >= 2:
        second_name = parts[1]  # Second name
    if len(parts) >= 3:
        third_name = parts[2]  # Third name
    if len(parts) >= 4:
        fourth_name = parts[3]  # Fourth name
    if len(parts) >= 5:
        # If more than 4 parts, concatenate all additional names to LastName
        last_name = ' '.join(parts[4:])
    
    return pd.Series([first_name, second_name, third_name, fourth_name, last_name])
        
# Apply function to the DataFrame
GenderData[['FirstName', 'SecondName', 'ThirdName', 'FourthName', 'LastName']] = GenderData['CustomerName'].apply(split_name)


GenderData.head(40)


GenderData.isna().sum() #check null value


GenderData = GenderData.drop('CustomerName', axis=1) #drop column CustomerName


GenderData.head(40)





GenderData.to_csv("KGGenderDatasetClean.csv", index=False) #save clean dataset





import pandas as pd

GenderData = pd.read_csv('KGGenderDatasetClean.csv')
GenderData.head()





from sklearn.model_selection import train_test_split

# Function to extract bi-grams from a name
def get_bigrams(name):
    return tuple(name[i:i+2] for i in range(len(name)-1)) if isinstance(name, str) else ("",)

# Apply bi-gram tokenization to all name columns
X = GenderData[['FirstName', 'SecondName', 'ThirdName', 'FourthName', 'LastName']].astype(str)
X = X.map(get_bigrams)  

# Convert to categorical type
for col in X.columns:
    X[col] = X[col].astype("category")

# Encode y: M → 0, F → 1
y = GenderData['Gender'].map({'M': 0, 'F': 1})

# Splitting the dataset (75% train, 25% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)

# Print example data
print("Example of tokenized and categorized X_train:")
print(X_train.head())

print("\nExample of encoded y_train:")
print(y_train.head())






import numpy as np
import pandas as pd
from scipy.stats import chi2_contingency
from sklearn.utils import resample
from collections import Counter
from random import sample

class CHAIDTree:
    def __init__(self, min_samples_split=10, significance_level=0.05):
        self.min_samples_split = min_samples_split
        self.significance_level = significance_level
        self.tree = None  # Tree structure will be stored here

    def fit(self, X, y):
        self.tree = self._build_tree(X, y)

    def _build_tree(self, X, y):
        if len(y) < self.min_samples_split or len(set(y)) == 1:
            return Counter(y).most_common(1)[0][0]  # Return majority class

        best_split = self._find_best_split(X, y)
        if best_split is None:
            return Counter(y).most_common(1)[0][0]  # No valid split found
        
        feature, groups = best_split
        node = {"feature": feature, "children": {}}

        for category, (X_sub, y_sub) in groups.items():
            node["children"][category] = self._build_tree(X_sub, y_sub)

        return node

    def _find_best_split(self, X, y):
        best_feature = None
        best_p_value = float("inf")
        best_groups = None

        for feature in X.columns:
            categories = X[feature].unique()
            contingency_table = np.array([[(y[X[feature] == cat] == 0).sum(), 
                                           (y[X[feature] == cat] == 1).sum()] 
                                          for cat in categories])
            
            chi2, p, _, _ = chi2_contingency(contingency_table, correction=False)
            
            if p < self.significance_level and p < best_p_value:
                best_feature = feature
                best_p_value = p
                best_groups = {cat: (X[X[feature] == cat], y[X[feature] == cat]) for cat in categories}

        return (best_feature, best_groups) if best_feature else None

    def predict_sample(self, sample, node, y):
        if isinstance(node, (int, np.int64)):  # Leaf node
            return node
        feature = node["feature"]
        value = sample.get(feature, None)
        return self.predict_sample(sample, node["children"].get(value, Counter(y).most_common(1)[0][0]), y)

    def predict(self, X, y):
        return np.array([self.predict_sample(sample, self.tree, y) for _, sample in X.iterrows()])


class RandomForestCHAID:
    def __init__(self, n_trees=100, sample_size=0.8, min_samples_split=10, significance_level=0.05, max_features="sqrt"):
        self.n_trees = n_trees
        self.sample_size = sample_size
        self.min_samples_split = min_samples_split
        self.significance_level = significance_level
        self.max_features = max_features  # Feature bagging method
        self.trees = []
        self.selected_features = []  # Track selected features per tree

    def fit(self, X, y):
        n_samples = int(self.sample_size * len(X))
        n_features = self._calculate_n_features(X)  # Determine number of features per tree

        for _ in range(self.n_trees):
            # Bootstrap sample the dataset
            X_sample, y_sample = resample(X, y, n_samples=n_samples, random_state=None)

            # Feature Bagging: Select a random subset of features
            selected_features = tuple(sample(list(X.columns), n_features))
            self.selected_features.append(selected_features)  

            # Train a CHAID Decision Tree using only the selected features
            tree = CHAIDTree(min_samples_split=self.min_samples_split, significance_level=self.significance_level)
            tree.fit(X_sample[list(selected_features)], y_sample)  

            self.trees.append(tree)

    def _calculate_n_features(self, X):
        """ Determines number of features to use for feature bagging """
        if self.max_features == "sqrt":
            return max(1, int(np.sqrt(len(X.columns))))
        elif self.max_features == "log2":
            return max(1, int(np.log2(len(X.columns))))
        elif isinstance(self.max_features, int):
            return min(len(X.columns), self.max_features)
        else:
            return len(X.columns)  # Default: Use all features (no feature bagging)

    def predict(self, X):
        tree_preds = np.array([tree.predict(X[list(features)], y_train) 
                               for tree, features in zip(self.trees, self.selected_features)])

        # Majority Voting
        final_preds = [Counter(tree_preds[:, i]).most_common(1)[0][0] for i in range(X.shape[0])]
        return np.array(final_preds)


# Train Random Forest CHAID with Feature Bagging
rf = RandomForestCHAID(n_trees=100, sample_size=0.8, max_features="sqrt")  # Use sqrt(feature count)
rf.fit(X_train, y_train)

# Make predictions
y_pred = rf.predict(X_test)

# Evaluate accuracy
from sklearn.metrics import accuracy_score
print("Random Forest CHAID Accuracy:", accuracy_score(y_test, y_pred))









import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

GenderData = pd.read_csv('KGGenderDatasetClean.csv')
GenderData.head()

# Function to extract bi-grams from a name
def get_bigrams(name):
    return tuple(name[i:i+2] for i in range(len(name)-1)) if isinstance(name, str) else ("",)

# Apply bi-gram tokenization to all name columns
X = GenderData[['FirstName', 'SecondName', 'ThirdName', 'FourthName', 'LastName']].astype(str)
X = X.map(get_bigrams)  

# Encode y: M → 0, F → 1
y = GenderData['Gender'].map({'M': 0, 'F': 1}).values

# Build vocabulary for bi-grams
vocab = set()
for col in X.columns:
    for bi_grams in X[col]:
        vocab.update(bi_grams)

# Assign unique indices to bi-grams
bi_gram_to_index = {bi_gram: i for i, bi_gram in enumerate(vocab)}

# Hyperparameters
embedding_dim = 8  # Size of each embedding vector
vocab_size = len(bi_gram_to_index)

# Initialize embedding matrix with small random values
embedding_matrix =  np.random.rand(vocab_size, embedding_dim)

# Function to encode names into fixed-size vectors using embeddings
def encode_names(X):
    encoded_features = []
    
    for _, row in X.iterrows():
        name_embeddings = []
        
        for bi_grams in row:
            bi_gram_vectors = [embedding_matrix[bi_gram_to_index[bg]] for bg in bi_grams if bg in bi_gram_to_index]
            if bi_gram_vectors:
                avg_vector = np.mean(bi_gram_vectors, axis=0)  # Average embeddings
            else:
                avg_vector = np.zeros(embedding_dim)  # If no valid bi-grams, use zero vector
            
            name_embeddings.append(avg_vector)
        
        # Concatenate all name embeddings into a single vector
        encoded_features.append(np.concatenate(name_embeddings, axis=0))
    
    return np.array(encoded_features)

# Encode input data
X_encoded = encode_names(X)
X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.20)

print("Example of tokenized and categorized X_train:")
print(X_train[:5])  # Prints the first 5 rows

print("\nExample of encoded y_train:")
print(y_train[:5])  # Prints the first 5 labels






import numpy as np
from collections import defaultdict
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns
import pickle

# Activation functions
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def relu(x):
    return np.maximum(0, x)

def relu_derivative(x):
    return (x > 0).astype(float)

def sigmoid_derivative(x):
    return x * (1 - x)

# Define the Neural Network class
class NeuralNetwork:
    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):
        # Initialize weights and biases
        self.weights_input_hidden1 = np.random.randn(input_size, hidden_size1) * 0.01
        self.weights_hidden1_hidden2 = np.random.randn(hidden_size1, hidden_size2) * 0.01
        self.weights_hidden2_output = np.random.randn(hidden_size2, output_size) * 0.01
        
        self.bias_hidden1 = np.zeros((1, hidden_size1))
        self.bias_hidden2 = np.zeros((1, hidden_size2))
        self.bias_output = np.zeros((1, output_size))
        
    def forward(self, X):
        """ Forward pass with ReLU in hidden layers and Sigmoid in output layer """
        self.hidden1_input = np.dot(X, self.weights_input_hidden1) + self.bias_hidden1
        self.hidden1_output = relu(self.hidden1_input)  # ReLU Activation
        
        self.hidden2_input = np.dot(self.hidden1_output, self.weights_hidden1_hidden2) + self.bias_hidden2
        self.hidden2_output = relu(self.hidden2_input)  # ReLU Activation
        
        self.output_layer_input = np.dot(self.hidden2_output, self.weights_hidden2_output) + self.bias_output
        self.output = sigmoid(self.output_layer_input)  # Sigmoid for binary classification
        
        return self.output
    
    def backward(self, X, y, learning_rate):
        """ Backpropagation for updating weights and biases """
        for i in range(X.shape[0]):  # Stochastic Gradient Descent (SGD)
            x_sample = X[i:i+1]
            y_sample = y[i:i+1]

            # Forward pass
            output = self.forward(x_sample)
            
            # Compute errors
            output_error = y_sample - output
            output_delta = output_error * sigmoid_derivative(output)
            
            hidden2_error = output_delta.dot(self.weights_hidden2_output.T)
            hidden2_delta = hidden2_error * relu_derivative(self.hidden2_output)
            
            hidden1_error = hidden2_delta.dot(self.weights_hidden1_hidden2.T)
            hidden1_delta = hidden1_error * relu_derivative(self.hidden1_output)
            
            # Update weights and biases
            self.weights_hidden2_output += self.hidden2_output.T.dot(output_delta) * learning_rate
            self.bias_output += np.sum(output_delta, axis=0, keepdims=True) * learning_rate
            
            self.weights_hidden1_hidden2 += self.hidden1_output.T.dot(hidden2_delta) * learning_rate
            self.bias_hidden2 += np.sum(hidden2_delta, axis=0, keepdims=True) * learning_rate
            
            self.weights_input_hidden1 += x_sample.T.dot(hidden1_delta) * learning_rate
            self.bias_hidden1 += np.sum(hidden1_delta, axis=0, keepdims=True) * learning_rate
    
    def train(self, X, y, epochs, learning_rate):
        for epoch in range(epochs):
            self.backward(X, y, learning_rate)
            loss = np.mean(np.square(y - self.forward(X)))  # Mean Squared Error (MSE)
            print(f'Epoch {epoch}, Loss: {loss:.6f}')



def evaluate_model(nn, X, y, dataset_name):
    predictions = nn.forward(X)
    predicted_classes = (predictions > 0.5).astype(int)  # Convert sigmoid output to binary (0 or 1)

    # Compute metrics
    accuracy = accuracy_score(y, predicted_classes)
    precision = precision_score(y, predicted_classes, pos_label=1)  # For F (1)
    recall = recall_score(y, predicted_classes, pos_label=1)  # For F (1)
    f1 = f1_score(y, predicted_classes, pos_label=1)  # For F (1)
    
    # Compute ROC Curve and AUC
    fpr, tpr, _ = roc_curve(y, predictions)  # Use raw sigmoid outputs for ROC
    roc_auc = auc(fpr, tpr)

    # Save metrics
    metrics = {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'roc_auc': roc_auc
    }

    with open(f'{dataset_name}_metrics.pkl', 'wb') as f:
        pickle.dump(metrics, f)
    
    print(f"{dataset_name} Metrics: {metrics}")

    # Compute confusion matrix
    cm = confusion_matrix(y, predicted_classes)

    # Reordering confusion matrix to match TP for F (1)
    cm_reordered = np.array([[cm[1,1], cm[1,0]],  # TP (F), FN (M)
                              [cm[0,1], cm[0,0]]]) # FP (F), TN (M)

    # Plot confusion matrix
    plt.figure(figsize=(5, 4))
    sns.heatmap(cm_reordered, annot=True, fmt="d", cmap="Blues", 
                xticklabels=["Predicted F", "Predicted M"], 
                yticklabels=["Actual F", "Actual M"])
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.title(f"Confusion Matrix for {dataset_name}")
    plt.show()

    # Plot ROC Curve
    plt.figure(figsize=(6, 5))
    plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Diagonal reference line
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'ROC Curve for {dataset_name}')
    plt.legend(loc="lower right")
    plt.show()


# Create and train the Neural Network
input_size = X_train.shape[1]  # Match number of features
nn = NeuralNetwork(input_size=input_size, hidden_size1=128, hidden_size2=64, output_size=1)

# Train the neural network
nn.train(X_train, y_train, epochs=5, learning_rate=0.0001)


# Evaluate on training data
evaluate_model(nn, X_train, y_train, "train")

# Evaluate on testing data
evaluate_model(nn, X_test, y_test, "test")




